# ğŸ£ Mini GCP (Local Data Pipeline)

This project simulates a **modern data pipeline** architecture, entirely **locally**. It follows a modular design to extract, transform, load, validate, and analyze synthetic sales data using Python, Apache Beam, DuckDB, and PostgreSQL.

---

## ğŸ§  Why "Mini GCP"?

This repo mimics a GCP-like modular pipeline (with stages like Cloud Functions, Dataflow, BigQuery), but:

- ğŸ†“ Works locally

- ğŸ§ª Ideal for testing and learning

- ğŸ’¸ No cloud cost involved

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dags/                # Pipeline where we run all the tasks
â”œâ”€â”€ data/                # Data storage layer (raw, processed, validated, warehouse)
â”œâ”€â”€ export/              # Optional: export to PostgreSQL
â”œâ”€â”€ extract/             # Data extraction logic
â”œâ”€â”€ functions/           # Trigger logic (e.g., on new file)
â”œâ”€â”€ initial_validation/  # Initial quality validation
â”œâ”€â”€ load/                # Load cleaned data into DuckDB
â”œâ”€â”€ scripts/             # Automation scripts
â”œâ”€â”€ transform/           # Data transformation using Apache Beam
â”œâ”€â”€ validate/            # Schema and quality validation
â”œâ”€â”€ README.md            # You are here!
â””â”€â”€ requirements.txt     # Python dependencies
```

---

## ğŸ“Š Data Folder Overview

- **data/raw/:** Initial synthetic data (e.g., raw_sales_1.csv)

- **data/extract/:** Copied file ready for transformation

- **data/initial_validation/:** validation raw reports in .csv and .json

- **data/processed/:** Cleaned file after transformation

- **data/validation/:** Schema reports in .csv and .json

- **data/warehouse/:** Final data stored in DuckDB (sales.duckdb)

---

## âš™ï¸ Tools & Libraries

- ğŸ Python
- ğŸ¦† [DuckDB](https://duckdb.org/)
- âš™ï¸ [Apache Beam](https://beam.apache.org/)
- ğŸ§ª `pyspark`, `Faker`, `unidecode`, `watchdog`, `apache-beam[gcp]`, `pandas`, `python-dotenv`, `duckdb`, `sqlalchemy`, `psycopg2-binary`
- ğŸ˜ PostgreSQL
- ğŸ“¦ Local folders instead of cloud storage

---

## âœ… Requirements

Install dependencies:

```
pip install -r requirements.txt
```

---

## ğŸš§ Future Improvements

- [X] Add Airflow DAG for orchestration

- [ ] Create Looker Studio or local dashboards

- [ ] Extend validation rules with Great Expectations

---

## ğŸ“¬ Feedback or Questions?

Feel free to open an issue or submit a PR!

---

## ğŸ‘©â€ğŸ’» Author
Camila MuÃ±oz â€“ @CamilaJaviera91
ğŸ’¬ Happy to connect and discuss data pipelines or open source!

---