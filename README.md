# ğŸ£ Mini GCP (Local Data Pipeline)

This project simulates a **modern data pipeline** architecture, entirely **locally**. It follows a modular design to extract, transform, load, validate, and analyze synthetic sales data using Python, Apache Beam, DuckDB, and PostgreSQL.

---

## ğŸ“‘ Table of Contents

- [ğŸ£ Mini GCP (Local Data Pipeline)](#-mini-gcp-local-data-pipeline)
  - [ğŸ§  Why "Mini GCP"?](#-why-mini-gcp)
  - [âš¡ Quickstart](#-quickstart)
  - [ğŸ”„ Pipeline Flow](#-pipeline-flow)
  - [ğŸ“ Project Structure](#-project-structure)
  - [ğŸ“Š Data Folder Overview](#-data-folder-overview)
  - [âš™ï¸ Tools & Libraries](#ï¸-tools--libraries)
  - [âœ… Requirements](#-requirements)
    - [ğŸ Creating a Python Virtual Environment in VS Code](#-creating-a-python-virtual-environment-in-vs-code)
    - [âš™ï¸ Project Setup & Maintenance Scripts](#ï¸-project-setup--maintenance-scripts)
  - [âš™ï¸ Environment & Docker Notes](#ï¸-environment--docker-notes)
  - [ğŸš§ Future Improvements](#-future-improvements)
  - [ğŸ“Š LookerStudio & GoogleSheets](#ï¸-lookerStudio--googleSheets)
  - [ğŸ“¬ Feedback or Questions?](#-feedback-or-questions)
  - [ğŸ¤ Contributing](#-contributing)
  - [ğŸ‘©â€ğŸ’» Author](#-author)
  - [ğŸ“œ License](#-license)

---

## ğŸ§  Why "Mini GCP"?

This repo mimics a GCP-like modular pipeline (with stages like Cloud Functions, Dataflow, BigQuery), but:

- ğŸ†“ Works locally

- ğŸ§ª Ideal for testing and learning

- ğŸ’¸ No cloud cost involved

---

## âš¡ Quickstart

- Clone the repository and set up the environment:

```
git clone https://github.com/your-repo/mini-gcp.git
cd mini-gcp
```

---

## ğŸ”„ Pipeline Flow

```
raw data â†’ extract â†’ initial_validation â†’ transform â†’ load â†’ final_validation â†’ warehouse
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dags/                # Pipeline where we run all the tasks
â”œâ”€â”€ data/                # Data storage layer (raw, processed, validated, warehouse)
â”œâ”€â”€ export/              # Optional: export to PostgreSQL
â”œâ”€â”€ extract/             # Data extraction logic
â”œâ”€â”€ functions/           # Trigger logic (e.g., on new file)
â”œâ”€â”€ initial_validation/  # Initial quality validation
â”œâ”€â”€ load/                # Load cleaned data into DuckDB
â”œâ”€â”€ logs/                # Log the pipeline
â”œâ”€â”€ log_metadata/        # Log all metadata 
â”œâ”€â”€ scripts/             # Automation scripts
â”œâ”€â”€ transform/           # Data transformation using Apache Beam
â”œâ”€â”€ validate/            # Schema and quality validation
â”œâ”€â”€ README.md            # You are here!
â””â”€â”€ requirements.txt     # Python dependencies
```

---

## ğŸ“Š Data Folder Overview

- **data/raw/:** Initial synthetic data (e.g., raw_sales_1.csv)

- **data/extract/:** Copied file ready for transformation

- **data/initial_validation/:** validation reports generated from raw data.

- **data/processed/:** Cleaned file after transformation

- **data/validation/:** Schema reports in .csv and .json

- **data/warehouse/:** Final data stored in DuckDB (sales.duckdb)

---

## âš™ï¸ Tools & Libraries

- ğŸ [Python](https://www.python.org/)
- ğŸ¦† [DuckDB](https://duckdb.org/)
- âš™ï¸ [Apache Beam](https://beam.apache.org/)
- âš™ï¸ [Apache Airflow](https://airflow.apache.org/)
- ğŸ˜ [PostgreSQL](https://www.postgresql.org/)
- ğŸ‹ [Docker / Docker Compose](https://docs.docker.com/compose/)
- ğŸ§ª `pyspark`, `Faker`, `unidecode`, `watchdog`, `apache-beam[gcp]`, `pandas`, `python-dotenv`, `duckdb`, `sqlalchemy`, `psycopg2-binary`
- ğŸ“¦ Local folders instead of cloud storage

---

## âœ… Requirements

- Python 3.8+ (In this case we're usig 3.10)
- VS Code (Visual Studio Code)
- PostgreSQL
- DuckDB
- Apache Beam
- Git
- Docker
- DBeaver

### ğŸ Creating a Python Virtual Environment in VS Code

To keep dependencies isolated and avoid conflicts, itâ€™s a good practice to use a virtual environment for this project. Hereâ€™s how you can set it up in VS Code:

1. Open your project folder in VS Code.
2. Open the integrated terminal (Ctrl + ) or via menu: Terminal > New Terminal.
3. Create a virtual environment (only once):

- On Linux/macOS:

```
python3 -m venv .venv
```

- On Windows (PowerShell):

```
python -m venv .venv
```

- This creates a .venv folder inside your project with an isolated Python environment.

4. Activate the virtual environment:

- On Linux/macOS:

```
source .venv/bin/activate
```

- On Windows (PowerShell):

```
.venv\Scripts\Activate.ps1
```

- After activation, your terminal prompt will show the environment name, e.g., (.venv).

5. Install project dependencies inside the virtual environment:

```
pip install -r requirements.txt
```

6. (Optional) Select the interpreter in VS Code:

- Press `Ctrl + Shift + P` (or `Cmd + Shift + P` on macOS)

- Type and select Python: Select Interpreter

- Choose the interpreter from your `.venv` folder (it will show something like `.venv/bin/python` or `.venv\Scripts\python.exe`)

Now VS Code will use the virtual environmentâ€™s Python for running and debugging.

---

### âš™ï¸ Project Setup & Maintenance Scripts

This project includes a set of helper scripts to simplify initial setup, resetting the environment, and fixing file permissions when working with Airflow and Docker locally.

1. `1_init.sh` â€” Initialize Airflow âœˆï¸
This script builds the Airflow images and runs the Airflow initialization process inside Docker.

Usage:

```
chmod +x 1_init.sh
./1_init.sh
```

What it does:

- ğŸ—ï¸ Builds the `airflow-init` image without using Docker BuildKit (for better compatibility).

- ğŸ—„ï¸ Runs the `airflow-init` container to set up Airflowâ€™s metadata database and initial configuration.

2. `2_fix_permissions.sh` â€” Fix Project Folder Permissions ğŸ”
This script adjusts file and folder permissions so that the Airflow user inside the container (UID `50000`) can access and modify project files without permission errors.

Usage:

```
chmod +x 2_fix_permissions.sh
./2_fix_permissions.sh
```

What it does:

- ğŸ“‚ Iterates over key project folders (`dags`, `data`, `extract`, `transform`, etc.).

- ğŸ‘¤ Changes the owner to UID `50000` (Airflow user) and GID `0` (root group).

- ğŸ”’ Sets secure folder (`755`) and file (`644`) permissions.

Use this when:

- âš ï¸ You see permission denied errors when Airflow tries to read/write files.

3. `3_reset_docker.sh` â€” Reset Docker Environment ğŸ”„ğŸ³
This script completely cleans and rebuilds your local Docker setup for the project.

Usage:

```
chmod +x 3_reset_docker.sh
./3_reset_docker.sh
```

What it does:

- ğŸ›‘ Stops all running containers and removes volumes/orphan containers.

- ğŸ§¹ Prunes unused volumes and containers.

- ğŸ—ï¸ Rebuilds the projectâ€™s Docker image from scratch.

- ğŸš€ Starts services again with `docker compose up -d`.

Use this when:

- ğŸŒ± You want a fresh start.

- âš ï¸ There are issues with containers not working as expected.

#### ğŸ’¡ Tip:
If youâ€™re setting up the project for the first time, run the scripts in this order:

1. `1_init.sh` â†’ âœˆï¸ Initialize Airflow.

2. `2_fix_permissions.sh` â†’ ğŸ” Ensure correct file permissions.

3. (Optional) `3_reset_docker.sh` ğŸ”„ğŸ³ if you need a clean rebuild later.

---

## âš™ï¸ Environment & Docker Notes  

ğŸ’¡ **Important:**  
Every time you add a new variable in the `.env` file or edit the `docker-compose.yml` file, you need to **restart Docker** using the script:  

```
./3_reset_docker.sh
```

- This script stops and removes running containers, rebuilds the images, and restarts everything from scratch.

---

## ğŸš§ Future Improvements

- [X] Add Airflow DAG for orchestration

- [X] Create Looker Studio or local dashboards

- [X] Extend validation rules with Great Expectations

---

## ğŸ“Š LookerStudio & GoogleSheets:

- [GoogleSheets](https://docs.google.com/spreadsheets/d/178xzlNCJKyK7dmkyRhnoA7vkdypbRnbK14glFOHEODI/edit?gid=0#gid=0)

- [LookerStudio](https://lookerstudio.google.com/u/0/reporting/1ebab84e-02b6-4370-9691-83375c31a4cd/page/tEnnC)

---

## ğŸ“¬ Feedback or Questions?

Feel free to open an issue or submit a PR!

---

## ğŸ¤ Contributing

1. Fork the repo and create your branch:
```
git checkout -b feature/my-feature
```
2. Run tests and format code before pushing.
3. Submit a Pull Request ğŸš€.

---

## ğŸ‘©â€ğŸ’» Author
Camila MuÃ±oz â€“ [@CamilaJaviera91](https://github.com/CamilaJaviera91)
ğŸ’¬ Happy to connect and discuss data pipelines or open source!

---

## ğŸ“œ License
This project is licensed under the MIT License. See LICENSE for details.

---