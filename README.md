# üê£ Mini GCP (Local Data Pipeline)

This project simulates a **modern data pipeline** architecture, entirely **locally**. It follows a modular design to extract, transform, load, validate, and analyze synthetic sales data using Python, Apache Beam, DuckDB, and PostgreSQL.

---

## üß† Why "Mini GCP"?

This repo mimics a GCP-like modular pipeline (with stages like Cloud Functions, Dataflow, BigQuery), but:

- üÜì Works locally

- üß™ Ideal for testing and learning

- üí∏ No cloud cost involved

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ dags/                # Pipeline where we run all the tasks
‚îú‚îÄ‚îÄ data/                # Data storage layer (raw, processed, validated, warehouse)
‚îú‚îÄ‚îÄ export/              # Optional: export to PostgreSQL
‚îú‚îÄ‚îÄ extract/             # Data extraction logic
‚îú‚îÄ‚îÄ functions/           # Trigger logic (e.g., on new file)
‚îú‚îÄ‚îÄ initial_validation/  # Initial quality validation
‚îú‚îÄ‚îÄ load/                # Load cleaned data into DuckDB
‚îú‚îÄ‚îÄ scripts/             # Automation scripts
‚îú‚îÄ‚îÄ transform/           # Data transformation using Apache Beam
‚îú‚îÄ‚îÄ validate/            # Schema and quality validation
‚îú‚îÄ‚îÄ README.md            # You are here!
‚îî‚îÄ‚îÄ requirements.txt     # Python dependencies
```

---

## üìä Data Folder Overview

- **data/raw/:** Initial synthetic data (e.g., raw_sales_1.csv)

- **data/extract/:** Copied file ready for transformation

- **data/initial_validation/:** validation raw reports in .csv and .json

- **data/processed/:** Cleaned file after transformation

- **data/validation/:** Schema reports in .csv and .json

- **data/warehouse/:** Final data stored in DuckDB (sales.duckdb)

---

## ‚öôÔ∏è Tools & Libraries

- üêç Python
- ü¶Ü [DuckDB](https://duckdb.org/)
- ‚öôÔ∏è [Apache Beam](https://beam.apache.org/)
- üß™ `pyspark`, `Faker`, `unidecode`, `watchdog`, `apache-beam[gcp]`, `pandas`, `python-dotenv`, `duckdb`, `sqlalchemy`, `psycopg2-binary`
- üêò PostgreSQL
- üì¶ Local folders instead of cloud storage

---

## ‚úÖ Requirements

- Python 3.8+ (In this case we're usin 3.10)
- VS Code (Visual Studio Code)
- PostgreSQL
- DuckDB
- Apache Beam
- Git
- Docker
- DBeaver

### üêç Creating a Python Virtual Environment in VS Code

To keep dependencies isolated and avoid conflicts, it‚Äôs a good practice to use a virtual environment for this project. Here‚Äôs how you can set it up in VS Code:

1. Open your project folder in VS Code.
2. Open the integrated terminal (Ctrl + ) or via menu: Terminal > New Terminal.
3. Create a virtual environment (only once):

- On Linux/macOS:

```
python3 -m venv .venv
```

- On Windows (PowerShell):

```
python -m venv .venv
```

- This creates a .venv folder inside your project with an isolated Python environment.

4. Activate the virtual environment:

- On Linux/macOS:

```
source .venv/bin/activate
```

- On Windows (PowerShell):

```
.venv\Scripts\Activate.ps1
```

- After activation, your terminal prompt will show the environment name, e.g., (.venv).

5. Install project dependencies inside the virtual environment:

```
pip install -r requirements.txt
```

6. (Optional) Select the interpreter in VS Code:

- Press `Ctrl + Shift + P` (or `Cmd + Shift + P` on macOS)

- Type and select Python: Select Interpreter

- Choose the interpreter from your `.venv` folder (it will show something like `.venv/bin/python` or `.venv\Scripts\python.exe`)

Now VS Code will use the virtual environment‚Äôs Python for running and debugging.

---

### ‚öôÔ∏è Project Setup & Maintenance Scripts

This project includes a set of helper scripts to simplify initial setup, resetting the environment, and fixing file permissions when working with Airflow and Docker locally.

1. `1_init.sh` ‚Äî Initialize Airflow
This script builds the Airflow images and runs the Airflow initialization process inside Docker.

Usage:

```
chmod +x 1_init.sh
./1_init.sh
```

What it does:

- Builds the `airflow-init` image without using Docker BuildKit (for better compatibility).

- Runs the `airflow-init` container to set up Airflow‚Äôs metadata database and initial configuration.

2. `2_reset_docker.sh` ‚Äî Reset Docker Environment
This script completely cleans and rebuilds your local Docker setup for the project.

Usage:

```
chmod +x 2_reset_docker.sh
./2_reset_docker.sh
```

What it does:

- Stops all running containers and removes volumes/orphan containers.

- Prunes unused volumes and containers.

- Rebuilds the project‚Äôs Docker image from scratch.

- Starts services again with `docker compose up -d`.

Use this when:

- You want a fresh start.

- There are issues with containers not working as expected.

3. `3_fix_permissions.sh` ‚Äî Fix Project Folder Permissions
This script adjusts file and folder permissions so that the Airflow user inside the container (UID `50000`) can access and modify project files without permission errors.

Usage:

```
chmod +x 3_fix_permissions.sh
./3_fix_permissions.sh
```

What it does:

- Iterates over key project folders (`dags`, `data`, `extract`, `transform`, etc.).

- Changes the owner to UID `50000` (Airflow user) and GID `0` (root group).

Sets secure folder (755) and file (644) permissions.

Use this when:

- You see permission denied errors when Airflow tries to read/write files.

---

## üöß Future Improvements

- [X] Add Airflow DAG for orchestration

- [ ] Create Looker Studio or local dashboards

- [ ] Extend validation rules with Great Expectations

---

## üì¨ Feedback or Questions?

Feel free to open an issue or submit a PR!

---

## üë©‚Äçüíª Author
Camila Mu√±oz ‚Äì @CamilaJaviera91
üí¨ Happy to connect and discuss data pipelines or open source!

---